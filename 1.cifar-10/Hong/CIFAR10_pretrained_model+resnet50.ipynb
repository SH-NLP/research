{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_pretrained_model+resnet50.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaLZtdZFrJYU"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as K\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrlhBivpySR0"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras import initializers\n",
        "\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_1Ueqaf90LO"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VloaGRzN937h",
        "outputId": "eff6cede-2bae-4a6d-8a51-e31e3dd808d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INNnPVLGyST4",
        "outputId": "fbe623e9-5587-41fa-e1cf-cd87f7b28683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJomfBvMySWK",
        "outputId": "9af9f111-b56d-459e-c3e5-e5e336c324c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Training data:\")\n",
        "print (\"Number of examples: \", X_train.shape[0])\n",
        "print (\"Number of channels:\",X_train.shape[3]) \n",
        "print (\"Image size:\", X_train.shape[1], X_train.shape[2])\n",
        "print\n",
        "print (\"Test data:\")\n",
        "print (\"Number of examples:\", X_test.shape[0])\n",
        "print (\"Number of channels:\", X_test.shape[3])\n",
        "print (\"Image size:\", X_test.shape[1], X_test.shape[2]) \n",
        "\n",
        "print(X_train.shape, X_train.dtype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data:\n",
            "Number of examples:  50000\n",
            "Number of channels: 3\n",
            "Image size: 32 32\n",
            "Test data:\n",
            "Number of examples: 10000\n",
            "Number of channels: 3\n",
            "Image size: 32 32\n",
            "(50000, 32, 32, 3) uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSWA08eKMZJ2",
        "outputId": "8d6d4201-0ee8-49d9-fa6f-306304241860",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "print (\"mean before normalization:\", np.mean(X_train)) \n",
        "print (\"std before normalization:\", np.std(X_train))\n",
        "\n",
        "mean=[0,0,0]\n",
        "std=[0,0,0]\n",
        "newX_train = np.ones(X_train.shape)\n",
        "newX_test = np.ones(X_test.shape)\n",
        "for i in range(3):\n",
        "    mean[i] = np.mean(X_train[:,:,:,i])\n",
        "    std[i] = np.std(X_train[:,:,:,i])\n",
        "    \n",
        "for i in range(3):\n",
        "    newX_train[:,:,:,i] = X_train[:,:,:,i] - mean[i]\n",
        "    newX_train[:,:,:,i] = newX_train[:,:,:,i] / std[i]\n",
        "    newX_test[:,:,:,i] = X_test[:,:,:,i] - mean[i]\n",
        "    newX_test[:,:,:,i] = newX_test[:,:,:,i] / std[i]\n",
        "        \n",
        "    \n",
        "X_train = newX_train\n",
        "X_test = newX_test\n",
        "\n",
        "print (\"mean after normalization:\", np.mean(X_train))\n",
        "print (\"std after normalization:\", np.std(X_train))\n",
        "print(X_train.max())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean before normalization: 120.70756512369792\n",
            "std before normalization: 64.1500758911213\n",
            "mean after normalization: 4.91799193961621e-17\n",
            "std after normalization: 0.9999999999999996\n",
            "2.126789409516928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4KSk1_zyqA7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8YNm3FHydnm"
      },
      "source": [
        "ResNet pretrained Model Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRM1iyUqySYG",
        "outputId": "1cd7a170-58a6-4527-8628-481a2b6a0f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "## back born\n",
        "resNet_base = tf.keras.applications.resnet50.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    # input_tensor=None,\n",
        "    input_shape=(224,224,3),\n",
        "    pooling='max',\n",
        "    classes=10\n",
        ")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-7d9a056d1682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## back born\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m resNet_base = tf.keras.applications.resnet18.ResNet18(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# input_tensor=None,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.applications' has no attribute 'resnet18'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnwpH7hWySaN",
        "outputId": "e45c012e-0e7b-49ff-8a26-a91ea8e47cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "resNet_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pool (GlobalMaxPooling2D)   (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZBTMnyHuvl_",
        "outputId": "3bfbca69-9651-40df-fa5c-573fcc42114b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load 한 모델의 레이어 숫자 \n",
        "len(resNet_base.layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2kJ85vYvequ",
        "outputId": "0c1a41e7-d194-49b8-987f-fb75e66a3fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 로드한 모델 내 학습되는 레이어 숫자 출력\n",
        "print(len(resNet_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uj16wwhvwhB"
      },
      "source": [
        "# 전체 레이어 학습설정\n",
        "resNet_base.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owJeXfcqv5Zx",
        "outputId": "e25a8c53-018c-47fb-a5c8-7084be2998b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 로드한 모델 내 학습되는 레이어 숫자 출력\n",
        "print(len(resNet_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkA-hld6vIg8"
      },
      "source": [
        "# 일부 layer trainable 설정\n",
        "\n",
        "train_target_layer = 5\n",
        "\n",
        "# 0 ~ 마지막 21번째 : 학습제외\n",
        "for layer in resNet_base.layers[0:-train_target_layer]:\n",
        "  layer.trainable = False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL3ahLfxwsy6",
        "outputId": "4d9fba29-40ab-43d5-bf23-7903ba9ba040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 로드한 모델 내 학습되는 레이어 숫자 출력\n",
        "print(len(resNet_base.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlMRkxVEyz9m"
      },
      "source": [
        "cifar10 model redefine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_TlzFVMyScT"
      },
      "source": [
        "## 모델\n",
        "input_Layer = tf.keras.layers.Input(shape=(32,32,3))\n",
        "\n",
        "# pretrained 모델 입력에 맞춰서 입력사이즈 변경\n",
        "#x = tf.keras.layers.ZeroPadding2D( (96,96), input_shape=(32,32,3))(input_Layer)\n",
        "x = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (224, 224)))(input_Layer)\n",
        "\n",
        "x = resNet_base(x)\n",
        "x=tf.keras.layers.Flatten()(x)\n",
        "#x= tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "Out_Layer= tf.keras.layers.Dense(10, activation='softmax', kernel_initializer='he_normal')(x)\n",
        "model = tf.keras.Model(inputs=[input_Layer], outputs=[Out_Layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEY-JS323rfx",
        "outputId": "4e9a7508-08a6-423d-c8ef-a6298b8a76d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(' 동결하기 전 conv_base를훈련되는 가중치의 수:',\n",
        "      len(model.trainable_weights))\n",
        "\n",
        "#resNet_base.trainable = True   ## True면 back의 초기값이 가지고있던 웨이트로해서 학습 False면 bakcborn뒤의 모델만 학습\n",
        "\n",
        "print('conv_base를 동결한 후 훈련되는 가중치의 수:',\n",
        "      len(model.trainable_weights))  ## dence layer가 2개 선언. 각 layer 마다 w,b 파라미터 2개가 있고, 2개의 레이어만 학습하므로 총 4개 파라미터만 학습\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 동결하기 전 conv_base를훈련되는 가중치의 수: 2\n",
            "conv_base를 동결한 후 훈련되는 가중치의 수: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkMx_80IytnA",
        "outputId": "7914e3a0-4574-4a38-a0b8-04f681ad41c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 23,608,202\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyjKX77N9t4U"
      },
      "source": [
        "# the save point\n",
        "if use_colab:\n",
        "    checkpoint_dir ='/content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1'\n",
        "else:\n",
        "    checkpoint_dir = 'pretrained_resnet50/exp1'\n",
        "\n",
        "if not os.path.isdir(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRE7pLar9t6l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cNh9sgTytpG"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer='adam',\n",
        "                  metrics='categorical_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgvJhlq9ytrk"
      },
      "source": [
        "import keras.utils\n",
        "\n",
        "one_hot_y_train = keras.utils.to_categorical(y_train, num_classes=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4DnBExZBDwi"
      },
      "source": [
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='val_loss',\n",
        "                                                 mode='auto',\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaO9U6w4yttu",
        "outputId": "bed7b091-e703-4523-a51e-7c88eab1a044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, one_hot_y_train, epochs=150, batch_size=1000, validation_split=0.2\n",
        "          ,callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            " 2/40 [>.............................] - ETA: 49s - loss: 1.6315 - categorical_accuracy: 0.4355WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0127s vs `on_train_batch_end` time: 1.3011s). Check your callbacks.\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.5867 - categorical_accuracy: 0.4500WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0103s vs `on_test_batch_end` time: 1.3009s). Check your callbacks.\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.5867 - categorical_accuracy: 0.4500 - val_loss: 1.5686 - val_categorical_accuracy: 0.4483\n",
            "Epoch 2/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.5240 - categorical_accuracy: 0.4738\n",
            "Epoch 00002: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.5240 - categorical_accuracy: 0.4738 - val_loss: 1.5115 - val_categorical_accuracy: 0.4752\n",
            "Epoch 3/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.4743 - categorical_accuracy: 0.4932\n",
            "Epoch 00003: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.4743 - categorical_accuracy: 0.4932 - val_loss: 1.4776 - val_categorical_accuracy: 0.4855\n",
            "Epoch 4/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.4382 - categorical_accuracy: 0.5066\n",
            "Epoch 00004: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.4382 - categorical_accuracy: 0.5066 - val_loss: 1.4442 - val_categorical_accuracy: 0.4986\n",
            "Epoch 5/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.4100 - categorical_accuracy: 0.5156\n",
            "Epoch 00005: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.4100 - categorical_accuracy: 0.5156 - val_loss: 1.4276 - val_categorical_accuracy: 0.4989\n",
            "Epoch 6/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.3816 - categorical_accuracy: 0.5260\n",
            "Epoch 00006: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.3816 - categorical_accuracy: 0.5260 - val_loss: 1.3974 - val_categorical_accuracy: 0.5173\n",
            "Epoch 7/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.3622 - categorical_accuracy: 0.5317\n",
            "Epoch 00007: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.3622 - categorical_accuracy: 0.5317 - val_loss: 1.3754 - val_categorical_accuracy: 0.5248\n",
            "Epoch 8/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.3381 - categorical_accuracy: 0.5408\n",
            "Epoch 00008: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.3381 - categorical_accuracy: 0.5408 - val_loss: 1.3692 - val_categorical_accuracy: 0.5205\n",
            "Epoch 9/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.3230 - categorical_accuracy: 0.5451\n",
            "Epoch 00009: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.3230 - categorical_accuracy: 0.5451 - val_loss: 1.3577 - val_categorical_accuracy: 0.5308\n",
            "Epoch 10/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.3040 - categorical_accuracy: 0.5537\n",
            "Epoch 00010: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.3040 - categorical_accuracy: 0.5537 - val_loss: 1.3448 - val_categorical_accuracy: 0.5301\n",
            "Epoch 11/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2881 - categorical_accuracy: 0.5603\n",
            "Epoch 00011: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2881 - categorical_accuracy: 0.5603 - val_loss: 1.3234 - val_categorical_accuracy: 0.5437\n",
            "Epoch 12/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2762 - categorical_accuracy: 0.5611\n",
            "Epoch 00012: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2762 - categorical_accuracy: 0.5611 - val_loss: 1.3255 - val_categorical_accuracy: 0.5412\n",
            "Epoch 13/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2613 - categorical_accuracy: 0.5700\n",
            "Epoch 00013: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2613 - categorical_accuracy: 0.5700 - val_loss: 1.3124 - val_categorical_accuracy: 0.5499\n",
            "Epoch 14/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2500 - categorical_accuracy: 0.5728\n",
            "Epoch 00014: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2500 - categorical_accuracy: 0.5728 - val_loss: 1.2973 - val_categorical_accuracy: 0.5536\n",
            "Epoch 15/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2474 - categorical_accuracy: 0.5740\n",
            "Epoch 00015: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2474 - categorical_accuracy: 0.5740 - val_loss: 1.2961 - val_categorical_accuracy: 0.5503\n",
            "Epoch 16/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2317 - categorical_accuracy: 0.5774\n",
            "Epoch 00016: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2317 - categorical_accuracy: 0.5774 - val_loss: 1.2873 - val_categorical_accuracy: 0.5595\n",
            "Epoch 17/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2281 - categorical_accuracy: 0.5791\n",
            "Epoch 00017: val_loss did not improve from 1.27880\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2281 - categorical_accuracy: 0.5791 - val_loss: 1.2878 - val_categorical_accuracy: 0.5578\n",
            "Epoch 18/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2156 - categorical_accuracy: 0.5851\n",
            "Epoch 00018: val_loss improved from 1.27880 to 1.27332, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2156 - categorical_accuracy: 0.5851 - val_loss: 1.2733 - val_categorical_accuracy: 0.5629\n",
            "Epoch 19/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.2023 - categorical_accuracy: 0.5899\n",
            "Epoch 00019: val_loss improved from 1.27332 to 1.26912, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.2023 - categorical_accuracy: 0.5899 - val_loss: 1.2691 - val_categorical_accuracy: 0.5627\n",
            "Epoch 20/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1995 - categorical_accuracy: 0.5899\n",
            "Epoch 00020: val_loss did not improve from 1.26912\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1995 - categorical_accuracy: 0.5899 - val_loss: 1.2716 - val_categorical_accuracy: 0.5633\n",
            "Epoch 21/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1925 - categorical_accuracy: 0.5912\n",
            "Epoch 00021: val_loss improved from 1.26912 to 1.25730, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1925 - categorical_accuracy: 0.5912 - val_loss: 1.2573 - val_categorical_accuracy: 0.5666\n",
            "Epoch 22/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1913 - categorical_accuracy: 0.5920\n",
            "Epoch 00022: val_loss did not improve from 1.25730\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1913 - categorical_accuracy: 0.5920 - val_loss: 1.2626 - val_categorical_accuracy: 0.5642\n",
            "Epoch 23/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1790 - categorical_accuracy: 0.5967\n",
            "Epoch 00023: val_loss improved from 1.25730 to 1.24866, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1790 - categorical_accuracy: 0.5967 - val_loss: 1.2487 - val_categorical_accuracy: 0.5698\n",
            "Epoch 24/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1737 - categorical_accuracy: 0.5998\n",
            "Epoch 00024: val_loss did not improve from 1.24866\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1737 - categorical_accuracy: 0.5998 - val_loss: 1.2515 - val_categorical_accuracy: 0.5708\n",
            "Epoch 25/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1663 - categorical_accuracy: 0.6024\n",
            "Epoch 00025: val_loss improved from 1.24866 to 1.24836, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1663 - categorical_accuracy: 0.6024 - val_loss: 1.2484 - val_categorical_accuracy: 0.5675\n",
            "Epoch 26/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1598 - categorical_accuracy: 0.6037\n",
            "Epoch 00026: val_loss improved from 1.24836 to 1.24770, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1598 - categorical_accuracy: 0.6037 - val_loss: 1.2477 - val_categorical_accuracy: 0.5733\n",
            "Epoch 27/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1588 - categorical_accuracy: 0.6038\n",
            "Epoch 00027: val_loss did not improve from 1.24770\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1588 - categorical_accuracy: 0.6038 - val_loss: 1.2505 - val_categorical_accuracy: 0.5692\n",
            "Epoch 28/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1620 - categorical_accuracy: 0.6006\n",
            "Epoch 00028: val_loss improved from 1.24770 to 1.24083, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1620 - categorical_accuracy: 0.6006 - val_loss: 1.2408 - val_categorical_accuracy: 0.5743\n",
            "Epoch 29/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1479 - categorical_accuracy: 0.6089\n",
            "Epoch 00029: val_loss improved from 1.24083 to 1.23451, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1479 - categorical_accuracy: 0.6089 - val_loss: 1.2345 - val_categorical_accuracy: 0.5757\n",
            "Epoch 30/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1467 - categorical_accuracy: 0.6059\n",
            "Epoch 00030: val_loss did not improve from 1.23451\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1467 - categorical_accuracy: 0.6059 - val_loss: 1.2375 - val_categorical_accuracy: 0.5712\n",
            "Epoch 31/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1408 - categorical_accuracy: 0.6096\n",
            "Epoch 00031: val_loss improved from 1.23451 to 1.22842, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1408 - categorical_accuracy: 0.6096 - val_loss: 1.2284 - val_categorical_accuracy: 0.5752\n",
            "Epoch 32/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1376 - categorical_accuracy: 0.6093\n",
            "Epoch 00032: val_loss did not improve from 1.22842\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1376 - categorical_accuracy: 0.6093 - val_loss: 1.2404 - val_categorical_accuracy: 0.5807\n",
            "Epoch 33/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1332 - categorical_accuracy: 0.6130\n",
            "Epoch 00033: val_loss did not improve from 1.22842\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1332 - categorical_accuracy: 0.6130 - val_loss: 1.2408 - val_categorical_accuracy: 0.5722\n",
            "Epoch 34/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1314 - categorical_accuracy: 0.6131\n",
            "Epoch 00034: val_loss improved from 1.22842 to 1.22609, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1314 - categorical_accuracy: 0.6131 - val_loss: 1.2261 - val_categorical_accuracy: 0.5810\n",
            "Epoch 35/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1248 - categorical_accuracy: 0.6159\n",
            "Epoch 00035: val_loss did not improve from 1.22609\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1248 - categorical_accuracy: 0.6159 - val_loss: 1.2440 - val_categorical_accuracy: 0.5682\n",
            "Epoch 36/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1224 - categorical_accuracy: 0.6166\n",
            "Epoch 00036: val_loss improved from 1.22609 to 1.22188, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1224 - categorical_accuracy: 0.6166 - val_loss: 1.2219 - val_categorical_accuracy: 0.5800\n",
            "Epoch 37/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1187 - categorical_accuracy: 0.6175\n",
            "Epoch 00037: val_loss did not improve from 1.22188\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1187 - categorical_accuracy: 0.6175 - val_loss: 1.2256 - val_categorical_accuracy: 0.5737\n",
            "Epoch 38/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1140 - categorical_accuracy: 0.6196\n",
            "Epoch 00038: val_loss improved from 1.22188 to 1.21417, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1140 - categorical_accuracy: 0.6196 - val_loss: 1.2142 - val_categorical_accuracy: 0.5847\n",
            "Epoch 39/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1150 - categorical_accuracy: 0.6176\n",
            "Epoch 00039: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1150 - categorical_accuracy: 0.6176 - val_loss: 1.2291 - val_categorical_accuracy: 0.5745\n",
            "Epoch 40/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1111 - categorical_accuracy: 0.6196\n",
            "Epoch 00040: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1111 - categorical_accuracy: 0.6196 - val_loss: 1.2146 - val_categorical_accuracy: 0.5864\n",
            "Epoch 41/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1088 - categorical_accuracy: 0.6200\n",
            "Epoch 00041: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1088 - categorical_accuracy: 0.6200 - val_loss: 1.2287 - val_categorical_accuracy: 0.5823\n",
            "Epoch 42/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1017 - categorical_accuracy: 0.6231\n",
            "Epoch 00042: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1017 - categorical_accuracy: 0.6231 - val_loss: 1.2264 - val_categorical_accuracy: 0.5813\n",
            "Epoch 43/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.1008 - categorical_accuracy: 0.6227\n",
            "Epoch 00043: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.1008 - categorical_accuracy: 0.6227 - val_loss: 1.2154 - val_categorical_accuracy: 0.5874\n",
            "Epoch 44/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0991 - categorical_accuracy: 0.6219\n",
            "Epoch 00044: val_loss did not improve from 1.21417\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0991 - categorical_accuracy: 0.6219 - val_loss: 1.2158 - val_categorical_accuracy: 0.5829\n",
            "Epoch 45/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0948 - categorical_accuracy: 0.6258\n",
            "Epoch 00045: val_loss improved from 1.21417 to 1.21066, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0948 - categorical_accuracy: 0.6258 - val_loss: 1.2107 - val_categorical_accuracy: 0.5865\n",
            "Epoch 46/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0957 - categorical_accuracy: 0.6254\n",
            "Epoch 00046: val_loss improved from 1.21066 to 1.20742, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0957 - categorical_accuracy: 0.6254 - val_loss: 1.2074 - val_categorical_accuracy: 0.5878\n",
            "Epoch 47/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0863 - categorical_accuracy: 0.6295\n",
            "Epoch 00047: val_loss improved from 1.20742 to 1.20739, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0863 - categorical_accuracy: 0.6295 - val_loss: 1.2074 - val_categorical_accuracy: 0.5876\n",
            "Epoch 48/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0855 - categorical_accuracy: 0.6286\n",
            "Epoch 00048: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0855 - categorical_accuracy: 0.6286 - val_loss: 1.2090 - val_categorical_accuracy: 0.5867\n",
            "Epoch 49/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0844 - categorical_accuracy: 0.6282\n",
            "Epoch 00049: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0844 - categorical_accuracy: 0.6282 - val_loss: 1.2096 - val_categorical_accuracy: 0.5863\n",
            "Epoch 50/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0840 - categorical_accuracy: 0.6302\n",
            "Epoch 00050: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0840 - categorical_accuracy: 0.6302 - val_loss: 1.2126 - val_categorical_accuracy: 0.5874\n",
            "Epoch 51/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0780 - categorical_accuracy: 0.6334\n",
            "Epoch 00051: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0780 - categorical_accuracy: 0.6334 - val_loss: 1.2133 - val_categorical_accuracy: 0.5900\n",
            "Epoch 52/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0810 - categorical_accuracy: 0.6309\n",
            "Epoch 00052: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0810 - categorical_accuracy: 0.6309 - val_loss: 1.2076 - val_categorical_accuracy: 0.5874\n",
            "Epoch 53/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0739 - categorical_accuracy: 0.6334\n",
            "Epoch 00053: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0739 - categorical_accuracy: 0.6334 - val_loss: 1.2129 - val_categorical_accuracy: 0.5858\n",
            "Epoch 54/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0750 - categorical_accuracy: 0.6334\n",
            "Epoch 00054: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0750 - categorical_accuracy: 0.6334 - val_loss: 1.2137 - val_categorical_accuracy: 0.5848\n",
            "Epoch 55/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0701 - categorical_accuracy: 0.6345\n",
            "Epoch 00055: val_loss did not improve from 1.20739\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0701 - categorical_accuracy: 0.6345 - val_loss: 1.2120 - val_categorical_accuracy: 0.5892\n",
            "Epoch 56/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0767 - categorical_accuracy: 0.6311\n",
            "Epoch 00056: val_loss improved from 1.20739 to 1.20579, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0767 - categorical_accuracy: 0.6311 - val_loss: 1.2058 - val_categorical_accuracy: 0.5874\n",
            "Epoch 57/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0657 - categorical_accuracy: 0.6368\n",
            "Epoch 00057: val_loss improved from 1.20579 to 1.20463, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0657 - categorical_accuracy: 0.6368 - val_loss: 1.2046 - val_categorical_accuracy: 0.5854\n",
            "Epoch 58/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0629 - categorical_accuracy: 0.6386\n",
            "Epoch 00058: val_loss improved from 1.20463 to 1.19968, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0629 - categorical_accuracy: 0.6386 - val_loss: 1.1997 - val_categorical_accuracy: 0.5915\n",
            "Epoch 59/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0634 - categorical_accuracy: 0.6348\n",
            "Epoch 00059: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0634 - categorical_accuracy: 0.6348 - val_loss: 1.2060 - val_categorical_accuracy: 0.5900\n",
            "Epoch 60/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0634 - categorical_accuracy: 0.6359\n",
            "Epoch 00060: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0634 - categorical_accuracy: 0.6359 - val_loss: 1.2029 - val_categorical_accuracy: 0.5887\n",
            "Epoch 61/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0589 - categorical_accuracy: 0.6377\n",
            "Epoch 00061: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0589 - categorical_accuracy: 0.6377 - val_loss: 1.2056 - val_categorical_accuracy: 0.5911\n",
            "Epoch 62/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0547 - categorical_accuracy: 0.6389\n",
            "Epoch 00062: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0547 - categorical_accuracy: 0.6389 - val_loss: 1.2041 - val_categorical_accuracy: 0.5848\n",
            "Epoch 63/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0608 - categorical_accuracy: 0.6367\n",
            "Epoch 00063: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0608 - categorical_accuracy: 0.6367 - val_loss: 1.2094 - val_categorical_accuracy: 0.5849\n",
            "Epoch 64/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0540 - categorical_accuracy: 0.6410\n",
            "Epoch 00064: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0540 - categorical_accuracy: 0.6410 - val_loss: 1.2063 - val_categorical_accuracy: 0.5875\n",
            "Epoch 65/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0555 - categorical_accuracy: 0.6398\n",
            "Epoch 00065: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0555 - categorical_accuracy: 0.6398 - val_loss: 1.2121 - val_categorical_accuracy: 0.5868\n",
            "Epoch 66/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0535 - categorical_accuracy: 0.6388\n",
            "Epoch 00066: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0535 - categorical_accuracy: 0.6388 - val_loss: 1.2046 - val_categorical_accuracy: 0.5850\n",
            "Epoch 67/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0499 - categorical_accuracy: 0.6398\n",
            "Epoch 00067: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0499 - categorical_accuracy: 0.6398 - val_loss: 1.2076 - val_categorical_accuracy: 0.5863\n",
            "Epoch 68/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0476 - categorical_accuracy: 0.6424\n",
            "Epoch 00068: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0476 - categorical_accuracy: 0.6424 - val_loss: 1.2002 - val_categorical_accuracy: 0.5888\n",
            "Epoch 69/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0502 - categorical_accuracy: 0.6405\n",
            "Epoch 00069: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0502 - categorical_accuracy: 0.6405 - val_loss: 1.2077 - val_categorical_accuracy: 0.5831\n",
            "Epoch 70/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0492 - categorical_accuracy: 0.6405\n",
            "Epoch 00070: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0492 - categorical_accuracy: 0.6405 - val_loss: 1.2042 - val_categorical_accuracy: 0.5894\n",
            "Epoch 71/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0425 - categorical_accuracy: 0.6422\n",
            "Epoch 00071: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0425 - categorical_accuracy: 0.6422 - val_loss: 1.2051 - val_categorical_accuracy: 0.5882\n",
            "Epoch 72/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0495 - categorical_accuracy: 0.6388\n",
            "Epoch 00072: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0495 - categorical_accuracy: 0.6388 - val_loss: 1.2274 - val_categorical_accuracy: 0.5850\n",
            "Epoch 73/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0395 - categorical_accuracy: 0.6461\n",
            "Epoch 00073: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0395 - categorical_accuracy: 0.6461 - val_loss: 1.2065 - val_categorical_accuracy: 0.5864\n",
            "Epoch 74/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0389 - categorical_accuracy: 0.6455\n",
            "Epoch 00074: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0389 - categorical_accuracy: 0.6455 - val_loss: 1.2049 - val_categorical_accuracy: 0.5915\n",
            "Epoch 75/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0367 - categorical_accuracy: 0.6445\n",
            "Epoch 00075: val_loss did not improve from 1.19968\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0367 - categorical_accuracy: 0.6445 - val_loss: 1.2054 - val_categorical_accuracy: 0.5903\n",
            "Epoch 76/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0336 - categorical_accuracy: 0.6477\n",
            "Epoch 00076: val_loss improved from 1.19968 to 1.19551, saving model to /content/drive/My Drive/0003. AICC 자료/001. Study 자료/Project_Code/CIFAR-10/pretrained_resnet50/exp1\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0336 - categorical_accuracy: 0.6477 - val_loss: 1.1955 - val_categorical_accuracy: 0.5934\n",
            "Epoch 77/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0345 - categorical_accuracy: 0.6471\n",
            "Epoch 00077: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0345 - categorical_accuracy: 0.6471 - val_loss: 1.2039 - val_categorical_accuracy: 0.5898\n",
            "Epoch 78/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0315 - categorical_accuracy: 0.6476\n",
            "Epoch 00078: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0315 - categorical_accuracy: 0.6476 - val_loss: 1.2145 - val_categorical_accuracy: 0.5850\n",
            "Epoch 79/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0332 - categorical_accuracy: 0.6454\n",
            "Epoch 00079: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0332 - categorical_accuracy: 0.6454 - val_loss: 1.2024 - val_categorical_accuracy: 0.5918\n",
            "Epoch 80/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0304 - categorical_accuracy: 0.6474\n",
            "Epoch 00080: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0304 - categorical_accuracy: 0.6474 - val_loss: 1.1998 - val_categorical_accuracy: 0.5904\n",
            "Epoch 81/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0295 - categorical_accuracy: 0.6482\n",
            "Epoch 00081: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0295 - categorical_accuracy: 0.6482 - val_loss: 1.1991 - val_categorical_accuracy: 0.5904\n",
            "Epoch 82/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0269 - categorical_accuracy: 0.6497\n",
            "Epoch 00082: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0269 - categorical_accuracy: 0.6497 - val_loss: 1.1969 - val_categorical_accuracy: 0.5930\n",
            "Epoch 83/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0278 - categorical_accuracy: 0.6495\n",
            "Epoch 00083: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0278 - categorical_accuracy: 0.6495 - val_loss: 1.2132 - val_categorical_accuracy: 0.5841\n",
            "Epoch 84/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0270 - categorical_accuracy: 0.6497\n",
            "Epoch 00084: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0270 - categorical_accuracy: 0.6497 - val_loss: 1.2095 - val_categorical_accuracy: 0.5886\n",
            "Epoch 85/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0265 - categorical_accuracy: 0.6486\n",
            "Epoch 00085: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0265 - categorical_accuracy: 0.6486 - val_loss: 1.2065 - val_categorical_accuracy: 0.5884\n",
            "Epoch 86/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0260 - categorical_accuracy: 0.6487\n",
            "Epoch 00086: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0260 - categorical_accuracy: 0.6487 - val_loss: 1.2025 - val_categorical_accuracy: 0.5922\n",
            "Epoch 87/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0206 - categorical_accuracy: 0.6528\n",
            "Epoch 00087: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0206 - categorical_accuracy: 0.6528 - val_loss: 1.2156 - val_categorical_accuracy: 0.5875\n",
            "Epoch 88/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0251 - categorical_accuracy: 0.6486\n",
            "Epoch 00088: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0251 - categorical_accuracy: 0.6486 - val_loss: 1.2030 - val_categorical_accuracy: 0.5896\n",
            "Epoch 89/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0178 - categorical_accuracy: 0.6520\n",
            "Epoch 00089: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0178 - categorical_accuracy: 0.6520 - val_loss: 1.2184 - val_categorical_accuracy: 0.5818\n",
            "Epoch 90/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0186 - categorical_accuracy: 0.6515\n",
            "Epoch 00090: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0186 - categorical_accuracy: 0.6515 - val_loss: 1.2020 - val_categorical_accuracy: 0.5957\n",
            "Epoch 91/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0170 - categorical_accuracy: 0.6531\n",
            "Epoch 00091: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0170 - categorical_accuracy: 0.6531 - val_loss: 1.1970 - val_categorical_accuracy: 0.5923\n",
            "Epoch 92/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0149 - categorical_accuracy: 0.6525\n",
            "Epoch 00092: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0149 - categorical_accuracy: 0.6525 - val_loss: 1.2022 - val_categorical_accuracy: 0.5922\n",
            "Epoch 93/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0160 - categorical_accuracy: 0.6534\n",
            "Epoch 00093: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0160 - categorical_accuracy: 0.6534 - val_loss: 1.2067 - val_categorical_accuracy: 0.5892\n",
            "Epoch 94/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0190 - categorical_accuracy: 0.6527\n",
            "Epoch 00094: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0190 - categorical_accuracy: 0.6527 - val_loss: 1.2057 - val_categorical_accuracy: 0.5889\n",
            "Epoch 95/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0164 - categorical_accuracy: 0.6523\n",
            "Epoch 00095: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0164 - categorical_accuracy: 0.6523 - val_loss: 1.2089 - val_categorical_accuracy: 0.5898\n",
            "Epoch 96/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0149 - categorical_accuracy: 0.6510\n",
            "Epoch 00096: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0149 - categorical_accuracy: 0.6510 - val_loss: 1.1981 - val_categorical_accuracy: 0.5944\n",
            "Epoch 97/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0110 - categorical_accuracy: 0.6527\n",
            "Epoch 00097: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0110 - categorical_accuracy: 0.6527 - val_loss: 1.1997 - val_categorical_accuracy: 0.5907\n",
            "Epoch 98/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0113 - categorical_accuracy: 0.6552\n",
            "Epoch 00098: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0113 - categorical_accuracy: 0.6552 - val_loss: 1.2107 - val_categorical_accuracy: 0.5925\n",
            "Epoch 99/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0098 - categorical_accuracy: 0.6535\n",
            "Epoch 00099: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0098 - categorical_accuracy: 0.6535 - val_loss: 1.2080 - val_categorical_accuracy: 0.5937\n",
            "Epoch 100/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0070 - categorical_accuracy: 0.6564\n",
            "Epoch 00100: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0070 - categorical_accuracy: 0.6564 - val_loss: 1.2061 - val_categorical_accuracy: 0.5901\n",
            "Epoch 101/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0064 - categorical_accuracy: 0.6551\n",
            "Epoch 00101: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0064 - categorical_accuracy: 0.6551 - val_loss: 1.2047 - val_categorical_accuracy: 0.5922\n",
            "Epoch 102/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0040 - categorical_accuracy: 0.6572\n",
            "Epoch 00102: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0040 - categorical_accuracy: 0.6572 - val_loss: 1.2042 - val_categorical_accuracy: 0.5913\n",
            "Epoch 103/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0084 - categorical_accuracy: 0.6534\n",
            "Epoch 00103: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0084 - categorical_accuracy: 0.6534 - val_loss: 1.2105 - val_categorical_accuracy: 0.5924\n",
            "Epoch 104/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0076 - categorical_accuracy: 0.6544\n",
            "Epoch 00104: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0076 - categorical_accuracy: 0.6544 - val_loss: 1.2066 - val_categorical_accuracy: 0.5922\n",
            "Epoch 105/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0093 - categorical_accuracy: 0.6538\n",
            "Epoch 00105: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0093 - categorical_accuracy: 0.6538 - val_loss: 1.2072 - val_categorical_accuracy: 0.5925\n",
            "Epoch 106/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0048 - categorical_accuracy: 0.6575\n",
            "Epoch 00106: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0048 - categorical_accuracy: 0.6575 - val_loss: 1.2140 - val_categorical_accuracy: 0.5873\n",
            "Epoch 107/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0072 - categorical_accuracy: 0.6533\n",
            "Epoch 00107: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0072 - categorical_accuracy: 0.6533 - val_loss: 1.2022 - val_categorical_accuracy: 0.5928\n",
            "Epoch 108/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0077 - categorical_accuracy: 0.6534\n",
            "Epoch 00108: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0077 - categorical_accuracy: 0.6534 - val_loss: 1.2036 - val_categorical_accuracy: 0.5911\n",
            "Epoch 109/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0062 - categorical_accuracy: 0.6556\n",
            "Epoch 00109: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0062 - categorical_accuracy: 0.6556 - val_loss: 1.2107 - val_categorical_accuracy: 0.5886\n",
            "Epoch 110/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0036 - categorical_accuracy: 0.6557\n",
            "Epoch 00110: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0036 - categorical_accuracy: 0.6557 - val_loss: 1.2027 - val_categorical_accuracy: 0.5937\n",
            "Epoch 111/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9985 - categorical_accuracy: 0.6582\n",
            "Epoch 00111: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9985 - categorical_accuracy: 0.6582 - val_loss: 1.1978 - val_categorical_accuracy: 0.5973\n",
            "Epoch 112/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9983 - categorical_accuracy: 0.6576\n",
            "Epoch 00112: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 65s 2s/step - loss: 0.9983 - categorical_accuracy: 0.6576 - val_loss: 1.2045 - val_categorical_accuracy: 0.5917\n",
            "Epoch 113/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0045 - categorical_accuracy: 0.6553\n",
            "Epoch 00113: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0045 - categorical_accuracy: 0.6553 - val_loss: 1.2095 - val_categorical_accuracy: 0.5897\n",
            "Epoch 114/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 1.0032 - categorical_accuracy: 0.6563\n",
            "Epoch 00114: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 1.0032 - categorical_accuracy: 0.6563 - val_loss: 1.2141 - val_categorical_accuracy: 0.5901\n",
            "Epoch 115/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9943 - categorical_accuracy: 0.6616\n",
            "Epoch 00115: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9943 - categorical_accuracy: 0.6616 - val_loss: 1.2051 - val_categorical_accuracy: 0.5927\n",
            "Epoch 116/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9936 - categorical_accuracy: 0.6618\n",
            "Epoch 00116: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9936 - categorical_accuracy: 0.6618 - val_loss: 1.2049 - val_categorical_accuracy: 0.5895\n",
            "Epoch 117/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9938 - categorical_accuracy: 0.6597\n",
            "Epoch 00117: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9938 - categorical_accuracy: 0.6597 - val_loss: 1.2121 - val_categorical_accuracy: 0.5892\n",
            "Epoch 118/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9955 - categorical_accuracy: 0.6582\n",
            "Epoch 00118: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9955 - categorical_accuracy: 0.6582 - val_loss: 1.2254 - val_categorical_accuracy: 0.5910\n",
            "Epoch 119/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9952 - categorical_accuracy: 0.6588\n",
            "Epoch 00119: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9952 - categorical_accuracy: 0.6588 - val_loss: 1.2094 - val_categorical_accuracy: 0.5924\n",
            "Epoch 120/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9923 - categorical_accuracy: 0.6633\n",
            "Epoch 00120: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9923 - categorical_accuracy: 0.6633 - val_loss: 1.2011 - val_categorical_accuracy: 0.5946\n",
            "Epoch 121/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9900 - categorical_accuracy: 0.6600\n",
            "Epoch 00121: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9900 - categorical_accuracy: 0.6600 - val_loss: 1.2022 - val_categorical_accuracy: 0.5958\n",
            "Epoch 122/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9856 - categorical_accuracy: 0.6631\n",
            "Epoch 00122: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9856 - categorical_accuracy: 0.6631 - val_loss: 1.2111 - val_categorical_accuracy: 0.5934\n",
            "Epoch 123/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9938 - categorical_accuracy: 0.6596\n",
            "Epoch 00123: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9938 - categorical_accuracy: 0.6596 - val_loss: 1.2105 - val_categorical_accuracy: 0.5932\n",
            "Epoch 124/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9876 - categorical_accuracy: 0.6621\n",
            "Epoch 00124: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9876 - categorical_accuracy: 0.6621 - val_loss: 1.2074 - val_categorical_accuracy: 0.5935\n",
            "Epoch 125/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9909 - categorical_accuracy: 0.6604\n",
            "Epoch 00125: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9909 - categorical_accuracy: 0.6604 - val_loss: 1.2051 - val_categorical_accuracy: 0.5947\n",
            "Epoch 126/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9892 - categorical_accuracy: 0.6618\n",
            "Epoch 00126: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9892 - categorical_accuracy: 0.6618 - val_loss: 1.2176 - val_categorical_accuracy: 0.5902\n",
            "Epoch 127/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9905 - categorical_accuracy: 0.6611\n",
            "Epoch 00127: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9905 - categorical_accuracy: 0.6611 - val_loss: 1.2197 - val_categorical_accuracy: 0.5867\n",
            "Epoch 128/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9921 - categorical_accuracy: 0.6602\n",
            "Epoch 00128: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9921 - categorical_accuracy: 0.6602 - val_loss: 1.2049 - val_categorical_accuracy: 0.5963\n",
            "Epoch 129/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9900 - categorical_accuracy: 0.6604\n",
            "Epoch 00129: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9900 - categorical_accuracy: 0.6604 - val_loss: 1.2135 - val_categorical_accuracy: 0.5916\n",
            "Epoch 130/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9848 - categorical_accuracy: 0.6635\n",
            "Epoch 00130: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9848 - categorical_accuracy: 0.6635 - val_loss: 1.2098 - val_categorical_accuracy: 0.5934\n",
            "Epoch 131/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9854 - categorical_accuracy: 0.6630\n",
            "Epoch 00131: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9854 - categorical_accuracy: 0.6630 - val_loss: 1.2199 - val_categorical_accuracy: 0.5902\n",
            "Epoch 132/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9839 - categorical_accuracy: 0.6633\n",
            "Epoch 00132: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9839 - categorical_accuracy: 0.6633 - val_loss: 1.2067 - val_categorical_accuracy: 0.5945\n",
            "Epoch 133/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9859 - categorical_accuracy: 0.6637\n",
            "Epoch 00133: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9859 - categorical_accuracy: 0.6637 - val_loss: 1.2216 - val_categorical_accuracy: 0.5908\n",
            "Epoch 134/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9852 - categorical_accuracy: 0.6630\n",
            "Epoch 00134: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9852 - categorical_accuracy: 0.6630 - val_loss: 1.2191 - val_categorical_accuracy: 0.5905\n",
            "Epoch 135/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9805 - categorical_accuracy: 0.6662\n",
            "Epoch 00135: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9805 - categorical_accuracy: 0.6662 - val_loss: 1.2098 - val_categorical_accuracy: 0.5914\n",
            "Epoch 136/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9812 - categorical_accuracy: 0.6636\n",
            "Epoch 00136: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9812 - categorical_accuracy: 0.6636 - val_loss: 1.2246 - val_categorical_accuracy: 0.5903\n",
            "Epoch 137/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9869 - categorical_accuracy: 0.6614\n",
            "Epoch 00137: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9869 - categorical_accuracy: 0.6614 - val_loss: 1.2134 - val_categorical_accuracy: 0.5899\n",
            "Epoch 138/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9830 - categorical_accuracy: 0.6631\n",
            "Epoch 00138: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9830 - categorical_accuracy: 0.6631 - val_loss: 1.2212 - val_categorical_accuracy: 0.5878\n",
            "Epoch 139/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9833 - categorical_accuracy: 0.6636\n",
            "Epoch 00139: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9833 - categorical_accuracy: 0.6636 - val_loss: 1.2140 - val_categorical_accuracy: 0.5915\n",
            "Epoch 140/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9788 - categorical_accuracy: 0.6654\n",
            "Epoch 00140: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9788 - categorical_accuracy: 0.6654 - val_loss: 1.2202 - val_categorical_accuracy: 0.5894\n",
            "Epoch 141/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9779 - categorical_accuracy: 0.6654\n",
            "Epoch 00141: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9779 - categorical_accuracy: 0.6654 - val_loss: 1.2168 - val_categorical_accuracy: 0.5914\n",
            "Epoch 142/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9811 - categorical_accuracy: 0.6633\n",
            "Epoch 00142: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9811 - categorical_accuracy: 0.6633 - val_loss: 1.2268 - val_categorical_accuracy: 0.5847\n",
            "Epoch 143/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9792 - categorical_accuracy: 0.6648\n",
            "Epoch 00143: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9792 - categorical_accuracy: 0.6648 - val_loss: 1.2120 - val_categorical_accuracy: 0.5941\n",
            "Epoch 144/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9772 - categorical_accuracy: 0.6662\n",
            "Epoch 00144: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9772 - categorical_accuracy: 0.6662 - val_loss: 1.2334 - val_categorical_accuracy: 0.5844\n",
            "Epoch 145/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9821 - categorical_accuracy: 0.6621\n",
            "Epoch 00145: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9821 - categorical_accuracy: 0.6621 - val_loss: 1.2246 - val_categorical_accuracy: 0.5901\n",
            "Epoch 146/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9805 - categorical_accuracy: 0.6638\n",
            "Epoch 00146: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9805 - categorical_accuracy: 0.6638 - val_loss: 1.2165 - val_categorical_accuracy: 0.5919\n",
            "Epoch 147/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9773 - categorical_accuracy: 0.6658\n",
            "Epoch 00147: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9773 - categorical_accuracy: 0.6658 - val_loss: 1.2164 - val_categorical_accuracy: 0.5888\n",
            "Epoch 148/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9742 - categorical_accuracy: 0.6670\n",
            "Epoch 00148: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9742 - categorical_accuracy: 0.6670 - val_loss: 1.2134 - val_categorical_accuracy: 0.5928\n",
            "Epoch 149/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9750 - categorical_accuracy: 0.6673\n",
            "Epoch 00149: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9750 - categorical_accuracy: 0.6673 - val_loss: 1.2255 - val_categorical_accuracy: 0.5911\n",
            "Epoch 150/150\n",
            "40/40 [==============================] - ETA: 0s - loss: 0.9736 - categorical_accuracy: 0.6669\n",
            "Epoch 00150: val_loss did not improve from 1.19551\n",
            "40/40 [==============================] - 66s 2s/step - loss: 0.9736 - categorical_accuracy: 0.6669 - val_loss: 1.2292 - val_categorical_accuracy: 0.5870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f27f624c630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZzF_Q1oytwB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0YgoUtYySec"
      },
      "source": [
        "model.save('cifar10_pretrain_resnet50.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3VQ7Hcsa3G2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYmL3eB0a3I-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvNa59VKa31Y"
      },
      "source": [
        "Test 결과분석 - Confusion Matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH_ur5R6a3K6"
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "#     print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu4AdbYVa3NL"
      },
      "source": [
        "y_pred=model_1.predict_classes(x_test)\n",
        "y_true=np.argmax(y_test,axis=1)\n",
        "\n",
        "#Compute the confusion matrix\n",
        "confusion_mtx=confusion_matrix(y_true,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1J8K1Pka3PD"
      },
      "source": [
        "class_names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv-Wh5Kxa3RF"
      },
      "source": [
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, classes=class_names,\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVcmJdTxbERh"
      },
      "source": [
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ors5_KBtbETt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zn95yOPbEWT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}